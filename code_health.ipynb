{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of General_Health: {0: 55954, 1: 35810, 2: 95364, 3: 11331, 4: 110395}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Random Forest Accuracy: 0.34668933139064273\n",
      "Best Random Forest F1 Score: 0.3511388765133881\n",
      "Best Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36      2238\n",
      "           1       0.22      0.24      0.23      1433\n",
      "           2       0.39      0.32      0.35      3814\n",
      "           3       0.14      0.28      0.19       453\n",
      "           4       0.42      0.38      0.40      4416\n",
      "\n",
      "    accuracy                           0.35     12354\n",
      "   macro avg       0.30      0.33      0.31     12354\n",
      "weighted avg       0.36      0.35      0.35     12354\n",
      "\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Gradient Boosting Accuracy: 0.3418326048243484\n",
      "Best Gradient Boosting F1 Score: 0.3447175839296761\n",
      "Best Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.48      0.39      2238\n",
      "           1       0.20      0.16      0.18      1433\n",
      "           2       0.40      0.29      0.34      3814\n",
      "           3       0.14      0.47      0.21       453\n",
      "           4       0.43      0.36      0.39      4416\n",
      "\n",
      "    accuracy                           0.34     12354\n",
      "   macro avg       0.30      0.35      0.30     12354\n",
      "weighted avg       0.37      0.34      0.34     12354\n",
      "\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best XGBoost Accuracy: 0.3354379148453942\n",
      "Best XGBoost F1 Score: 0.33644414467685263\n",
      "Best XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.51      0.40      2238\n",
      "           1       0.19      0.13      0.15      1433\n",
      "           2       0.40      0.28      0.33      3814\n",
      "           3       0.13      0.53      0.21       453\n",
      "           4       0.43      0.35      0.38      4416\n",
      "\n",
      "    accuracy                           0.34     12354\n",
      "   macro avg       0.30      0.36      0.30     12354\n",
      "weighted avg       0.37      0.34      0.34     12354\n",
      "\n",
      "Logistic Regression Accuracy: 0.3255625708272624\n",
      "Logistic Regression F1 Score: 0.32101646332353506\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.54      0.40      2238\n",
      "           1       0.16      0.04      0.06      1433\n",
      "           2       0.41      0.27      0.33      3814\n",
      "           3       0.12      0.57      0.19       453\n",
      "           4       0.42      0.33      0.37      4416\n",
      "\n",
      "    accuracy                           0.33     12354\n",
      "   macro avg       0.28      0.35      0.27     12354\n",
      "weighted avg       0.36      0.33      0.32     12354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\USER\\Downloads\\CVD_cleaned (1).csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['Checkup', 'Exercise', 'Sex', 'Age_Category', 'Smoking_History', 'Alcohol_Consumption']\n",
    "for column in categorical_columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Define features and target variable\n",
    "target_column = 'General_Health'\n",
    "X = data.drop(['General_Health', 'Heart_Disease', 'Skin_Cancer', 'Other_Cancer', 'Depression', 'Diabetes', 'Arthritis'], axis=1)\n",
    "y = data[target_column]\n",
    "\n",
    "# Encode the target variable if it is categorical\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Check class distribution of the target variable\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Class distribution of {target_column}: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# Reduce the dataset size for quicker iterations (using a subset of the data)\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample)\n",
    "\n",
    "# Apply SMOTE to balance the training dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train_resampled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    report = classification_report(y_test, predictions)\n",
    "    return accuracy, f1, report\n",
    "\n",
    "# Train and evaluate Random Forest with Randomized Search\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, n_iter=5, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "rf_accuracy, rf_f1, rf_report = evaluate_model(best_rf_model, X_test_selected, y_test)\n",
    "print(\"Best Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Best Random Forest F1 Score:\", rf_f1)\n",
    "print(\"Best Random Forest Classification Report:\\n\", rf_report)\n",
    "\n",
    "# Train and evaluate Gradient Boosting with Randomized Search\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=42), gb_params, n_iter=5, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "best_gb_model = random_search_gb.best_estimator_\n",
    "gb_accuracy, gb_f1, gb_report = evaluate_model(best_gb_model, X_test_selected, y_test)\n",
    "print(\"Best Gradient Boosting Accuracy:\", gb_accuracy)\n",
    "print(\"Best Gradient Boosting F1 Score:\", gb_f1)\n",
    "print(\"Best Gradient Boosting Classification Report:\\n\", gb_report)\n",
    "\n",
    "# Train and evaluate XGBoost with Randomized Search\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=False), xgb_params, n_iter=5, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search_xgb.fit(X_train_selected, y_train_resampled)\n",
    "best_xgb_model = random_search_xgb.best_estimator_\n",
    "xgb_accuracy, xgb_f1, xgb_report = evaluate_model(best_xgb_model, X_test_selected, y_test)\n",
    "print(\"Best XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"Best XGBoost F1 Score:\", xgb_f1)\n",
    "print(\"Best XGBoost Classification Report:\\n\", xgb_report)\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train_selected, y_train_resampled)\n",
    "lr_accuracy, lr_f1, lr_report = evaluate_model(lr_model, X_test_selected, y_test)\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
    "print(\"Logistic Regression F1 Score:\", lr_f1)\n",
    "print(\"Logistic Regression Classification Report:\\n\", lr_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
